id,title,authors,type,venue,year,acceptance,note,link,abstract,figure,caption2011a,Creating Contextual Help for GUIs Using Screenshots,Tom Yeh; Tsung-Hsiang Chang; Bo Xie; Greg Walsh; Krist Wongsuphasawat; Ivan Watkins; Man Huang; Larry Davis; Ben Bederson,Conference Paper,UIST,2011,26,,,"Contextual help is effective for learning how to use GUIs by showing instructions and highlights on the actual interface rather than in a separate viewer. However, end-users and third-party tech support typically cannot create contextual help to assist other users because it requires programming skill and source code access. We present a creation tool for contextual help that allows users to apply common com-puter skills-taking screenshots and writing simple scripts. We perform pixel analysis on screenshots to make this tool applicable to a wide range of applications and platforms without source code access. We evaluated the tool's usability with three groups of participants: developers, in-structors, and tech support. We further validated the ap-plicability of our tool with 60 real tasks supported by the tech support of a university campus.",SikuliGuide.png,"Creating contextual help for a GUI. The window in the far back is the interface of the International Children's Digital Library. A separate window in the front displays existing help content for this interface. Our tool enables help designers to create help content that can be presented contextually in the actual interface (i.e., yellow callout and red circle) by writing a simple script and taking screenshots, as shown in the editor below."2011b,Correlating the Visual Representation of User Interfaces with their Internal Structures and Metadata,Tsung-Hsiang Chang; Tom Yeh; Robert Miller,Conference Paper,UIST,2011,26,,,"Pixel-based methods are emerging as a new and promising way to develop new interaction techniques on top of existing user interfaces. However, in order to maintain platform independence, other available low-level information about GUI widgets, such as accessibility metadata, was neglected intentionally. In this paper, we present a hybrid framework, PAX, which associates the visual representation of user interfaces (i.e. the pixels) and their internal hierarchical metadata (i.e. the content, role, and value). We identify challenges to building such a framework. We also develop and evaluate two new algorithms for detecting text at arbitrary places on the screen, and for segmenting a text image into individual word blobs. Finally, we validate our framework in implementations of three applications. We enhance an existing pixel-based system, Sikuli Script, and preserve the readability of its script code at the same time. Further, we create two novel applications, Screen Search and Screen Copy, to demonstrate how PAX can be applied to development of desktop-level interactive systems.",PAX.png,"(LEFT) The internal structure of a GUI given by Accessibility APIs (AX) may not necessarily correspond to the actual visual representation of the GUI. Boxes above indicate the windows and widgets returned by AX even though they are not visible to users. (RIGHT) PAX combines pixels and Accessibility APIs for more accurate association between the visual representation and internal structure of a GUI. It filters accessibility information for only visible objects (red boxes) and also provides role, content, location, and size of objects detected by pixel-based methods (green boxes). "2011c,A Case for Query by Image and Text Content: Searching Computer Help Using Screenshots and Keywords,Tom Yeh; Brandyn White; Jose San Pedro; Boris Katz; Larry Davis,Conference Paper,WWW,2011,12.5,,http://dl.acm.org/citation.cfm?id=1963405.1963513,"The multimedia information retrieval community has dedicated extensive research effort to the problem of content-based image retrieval (CBIR). However, these systems find their main limitation in the difficulty of creating pictorial queries. As a result, few systems offer the option of querying by visual examples, and rely on automatic concept detection and tagging techniques to provide support for searching visual content using textual queries.<p>This paper proposes and studies a practical multimodal web search scenario, where CBIR fits intuitively to improve the retrieval of rich information queries. Many online articles contain useful know-how knowledge about computer applications. These articles tend to be richly illustrated by screenshots. We present a system to search for such software know-how articles that leverages the visual correspondences between screenshots. Users can naturally create pictorial queries simply by taking a screenshot of the application to retrieve a list of articles containing a matching screenshot.<p>We build a prototype comprising 150k articles that are classified into walkthrough, book, gallery, and general categories, and provide a comprehensive evaluation of this system, focusing on technical (accuracy of CBIR techniques) and usability (perceived system usefulness) aspects. We also consider the study of added value features of such a visual-supported search, including the ability to perform cross-lingual queries. We find that the system is able to retrieve matching screenshots for a wide variety of programs, across language boundaries, and provide subjectively more useful results than keyword-based web and image search engines.",SikuliSearch.png,2010a,VizWiz: Nearly Real-time Answers to Visual Questions,Jeffrey Bigham; Chandrika Jayant; Hanjie Ji; Greg Little; Andrew Miller; Robert Miller; Robin Miller; Aubrey Tatarowicz; Brandyn White; Samuel White; Tom Yeh,Conference Paper,UIST,2010,18,Best Paper Award,http://dl.acm.org/citation.cfm?id=1866029.1866080,"The lack of access to visual information like text labels, icons, and colors can cause frustration and decrease independence for blind people. Current access technology uses automatic approaches to address some problems in this space, but the technology is error-prone, limited in scope, and quite expensive. In this paper, we introduce VizWiz, a talking application for mobile phones that offers a new alternative to answering visual questions in nearly real-time - asking multiple people on the web. To support answering questions quickly, we introduce a general approach for intelligently recruiting human workers in advance called quikTurkit so that workers are available when new questions arrive. A field deployment with 11 blind participants illustrates that blind people can effectively use VizWiz to cheaply answer questions in their everyday lives, highlighting issues that automatic approaches will need to address to be useful. Finally, we illustrate the potential of using VizWiz as part of the participatory design of advanced tools by using it to build and evaluate VizWiz::LocateIt, an interactive mobile tool that helps blind people solve general visual search problems.",,2010b,Why Did the Person Cross the Road (There)? Scene Understanding using Probabilistic Logic Models and Common Sense Reasoning,Aniruddha Kembhavi; Tom Yeh; Larry Davis,Conference Paper,ECCV,2010,27.68,,http://dl.acm.org/citation.cfm?id=1888081,"We develop a video understanding system for scene elements, such as bus stops, crosswalks, and intersections, that are characterized more by qualitative activities and geometry than by intrinsic appearance. The domain models for scene elements are not learned from a corpus of video, but instead, naturally elicited by humans, and represented as probabilistic logic rules within a Markov Logic Network framework. Human elicited models, however, represent object interactions as they occur in the 3D world rather than describing their appearance projection in some specific 2D image plane. We bridge this gap by recovering qualitative scene geometry to analyze object interactions in the 3D world and then reasoning about scene geometry, occlusions and common sense domain knowledge using a set of meta-rules. The effectiveness of this approach is demonstrated on a set of videos of public spaces.",,2010c,GUI Testing Using Computer Vision,Tsung-Hsiang Chang; Tom Yeh; Robert Miller,Conference Paper,CHI,2010,22,,http://dl.acm.org/citation.cfm?id=1753326.1753555,"Testing a GUI's visual behavior typically requires human testers to interact with the GUI and to observe whether the expected results of interaction are presented. This paper presents a new approach to GUI testing using computer vision for testers to automate their tasks. Testers can write a visual test script that uses images to specify which GUI components to interact with and what visual feedback to be observed. Testers can also generate visual test scripts by demonstration. By recording both input events and screen images, it is possible to extract the images of components interacted with and the visual feedback seen by the demonstrator, and generate a visual test script automatically. We show that a variety of GUI behavior can be tested using this approach. Also, we show how this approach can facilitate good testing practices such as unit testing, regression testing, and test-driven development.",SikuliTest.png,GUI testing (left) traditionally requires human testers to operate the GUI and verify its behavior visually. Our new testing frame- work allows the testers to write visual scripts (right) to automate this labor-intensive task.2009a,Sikuli: Using GUI Screenshots for Search and Automation,Tom Yeh; Tsung-Hsiang Chang; Robert Miller,Conference Paper,UIST,2009,16.9,Best Paper Award,http://dl.acm.org/citation.cfm?id=1622213,"We present Sikuli, a visual approach to search and automation of graphical user interfaces using screenshots. Sikuli allows users to take a screenshot of a GUI element (such as a toolbar button, icon, or dialog box) and query a help system using the screenshot instead of the element's name. Sikuli also provides a visual scripting API for automating GUI interactions, using screenshot patterns to direct mouse and keyboard events. We report a web-based user study showing that searching by screenshot is easy to learn and faster to specify than keywords. We also demonstrate several automation tasks suitable for visual scripting, such as map navigation and bus tracking, and show how visual scripting can improve interactive help systems previously proposed in the literature.",Sikuli.png,Sikuli Search allows users to search docu- mentation and save custom annotations for a GUI element using its screenshot (captured by stretching a rectangle around it). Sikuli Script allows users to au- tomate GUI interactions also using screenshots.2009b,Fast Concurrent Object Localization and Recognition,Tom Yeh; John Lee; Trevor Darrell,Conference Paper,CVPR,2009,26.16,,http://dx.doi.org/10.1109/CVPR.2009.5206805,"Object localization and recognition are important problems in computer vision. However, in many applications, exhaustive search over all object models and image locations is computationally prohibitive. While several methods have been proposed to make either recognition or localization more efficient, few have dealt with both tasks simultaneously. This paper proposes an efficient method for concurrent object localization and recognition based on a data-dependent multi-class branch-and-bound formalism. Existing bag-of-features recognition techniques which can be expressed as weighted combinations of feature counts can be readily adapted to our method. We present experimental results that demonstrate the merit of our algorithm in terms of recognition accuracy, localization accuracy, and speed, compared to baseline approaches including exhaustive search, implicit-shape model (ISM), and efficient sub-window search (ESS). Moreover, we develop two extensions to consider non-rectangular bounding regions-composite boxes and polygons-and demonstrate their ability to achieve higher recognition scores compared to traditional rectangular bounding boxes.",,2008a,Photo-based Question Answering ,Tom Yeh; John Lee; Trevor Darrell,Conference Paper,ACM Multimedia,2008,17,,,,,2008b,Dynamic Visual Category Learning ,Tom Yeh; Trevor Darrell,Conference Paper,CVPR,2008,27.9,,,,,2007,Adaptive Vocabulary Forests for Dynamic Indexing and Category Learning ,Tom Yeh; John Lee; Trevor Darrell,Conference Paper,ICCV,2007,23.4,,,,,2004a,IDeixis: Image-based Deixis for Finding Location-Based Information ,Konrad Tollmar; Tom Yeh; Trevor Darrell ,Conference Paper,MobileHCI,2004,,,,,,2004b,Searching the Web with Mobile Images for Location Recognition ,Tom Yeh; Konrad Tollmar; Trevor Darrell ,Conference Paper,CVPR,2004,23.6,,,,,2011d,Co-designing Contextual Tutorials for Older Adults on Searching Health Information on the Internet,Bo Xie; Tom Yeh; Greg Walsh; Ivan Watkins; Man Huang,Short Paper,ASIS&T,2011,,,,"Older adults' e-health literacy, or the ability to access and use reliable health information through electronic sources, is generally low. Improving older adults' e-health literacy requires innovative instructional approaches and tools. An integrated e-tutorial displays instructions as an overlay on the actual Website. Evidence in the literature suggests an integrated e-tutorial is more effective than a paper- or video-based tutorial among younger people. Yet, relatively little is known about the effectiveness of an integrated e-tutorial on the older population. This exploratory study began to explore the applicability of an integrated e-tutorial to an older population, focusing on the content area of e-health literacy. A specific integrated e-tutorial, the Online Tutorial Overlay Presenter (OnTOP), was used to add an overlay to the NIHSeniorHealth.gov Website. Features of the overlay were examined thoroughly in seven 2-hour-long participatory design sessions with ten older adults during November 2010-March 2011. Several participatory design techniques were used to elicit participants' preferences for design features of the OnTOP tutorial. These techniques included drawing on the board, voice recording and integration, and peer instruction. Three major themes were identified, including: 1) using contextual cues, including gestural and auditory cues, to facilitate learning; 2) tailoring to accommodate the learner's literacy level; and 3) enhancing existing interfaces by adding multimedia cues (e.g., using images of everyday objects to replace unfamiliar abstract computer symbols). These findings helped improve the design features of the OnTOP. They also contribute to the multimedia learning literature by generating empirical evidence about the effects of multimedia learning among the previously understudied older adult population, and by raising interesting questions worthy of further examination.",,2008c,Multimodal Question Answering for Mobile Devices ,Tom Yeh; Trevor Darrell,Short Paper,IUI,2008,38,,,,,2005a,A Picture is Worth a Thousand Keywords: Image-based Object Search on a Mobile Platform ,Tom Yeh; Kristen Grauman; Konrad Tollmar; Trevor Darrell,Short Paper,CHI,2005,33,,,,,2005b,DoubleShot: An Interactive User-Aided Segmentation Tool ,Tom Yeh; Trevor Darrell,Short Paper,IUI,2005,34,,,,,2011e,Active Inference for Retrieval in Camera Networks,DaoZheng Chen; Mustafa Bilgic; Lise Getoor; David Jacobs; Lilly Mihalkova; Tom Yeh,Workshop,IEEE Workshop on Person Oriented Vision,2011,,,,,,2010d,Web-Scale Computer Vision using MapReduce for Multimedia Data Mining,Brandyn White; Tom Yeh; Jimmy Lin; Larry Davis,Workshop,KDD Multimedia Data Mining Workshop,2010,,,,,,2008d,Scalable Classifiers for Internet Vision Tasks ,Tom Yeh; John Lee; Trevor Darrell,Workshop,CVPR Internet Vision Workshop,2008,,,,,,2009c,"Searching Documentation Using Text, OCR, and Image",Tom Yeh; Boris Katz,Conference Poster,SIGIR,2009,34,,,,,2008e,Obtaining Help on GUI Elements using Screenshots ,Tom Yeh; Tsung-Hsiang Chang; Robert Miller,Conference Poster,UIST,2008,,,,,,