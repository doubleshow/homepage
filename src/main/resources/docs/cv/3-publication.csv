id,title,authors,type,venue,year,acceptance,note,link,abstract,figure,caption2011a,Creating Contextual Help for GUIs Using Screenshots,Tom Yeh; Tsung-Hsiang Chang; Bo Xie; Greg Walsh; Krist Wongsuphasawat; Ivan Watkins; Man Huang; Larry Davis; Ben Bederson,Conference Paper,UIST,2011,26,,,"Contextual help is effective for learning how to use GUIs by showing instructions and highlights on the actual interface rather than in a separate viewer. However, end-users and third-party tech support typically cannot create contextual help to assist other users because it requires programming skill and source code access. We present a creation tool for contextual help that allows users to apply common com-puter skills-taking screenshots and writing simple scripts. We perform pixel analysis on screenshots to make this tool applicable to a wide range of applications and platforms without source code access. We evaluated the tool's usability with three groups of participants: developers, in-structors, and tech support. We further validated the ap-plicability of our tool with 60 real tasks supported by the tech support of a university campus.",SikuliGuide.png,"Creating contextual help for a GUI. The window in the far back is the interface of the International Children's Digital Library. A separate window in the front displays existing help content for this interface. Our tool enables help designers to create help content that can be presented contextually in the actual interface (i.e., yellow callout and red circle) by writing a simple script and taking screenshots, as shown in the editor below."2011b,Correlating the Visual Representation of User Interfaces with their Internal Structures and Metadata,Tsung-Hsiang Chang; Tom Yeh; Robert Miller,Conference Paper,UIST,2011,26,,,"Pixel-based methods are emerging as a new and promising way to develop new interaction techniques on top of existing user interfaces. However, in order to maintain platform independence, other available low-level information about GUI widgets, such as accessibility metadata, was neglected intentionally. In this paper, we present a hybrid framework, PAX, which associates the visual representation of user interfaces (i.e. the pixels) and their internal hierarchical metadata (i.e. the content, role, and value). We identify challenges to building such a framework. We also develop and evaluate two new algorithms for detecting text at arbitrary places on the screen, and for segmenting a text image into individual word blobs. Finally, we validate our framework in implementations of three applications. We enhance an existing pixel-based system, Sikuli Script, and preserve the readability of its script code at the same time. Further, we create two novel applications, Screen Search and Screen Copy, to demonstrate how PAX can be applied to development of desktop-level interactive systems.",PAX.png,"(LEFT) The internal structure of a GUI given by Accessibility APIs (AX) may not necessarily correspond to the actual visual representation of the GUI. Boxes above indicate the windows and widgets returned by AX even though they are not visible to users. (RIGHT) PAX combines pixels and Accessibility APIs for more accurate association between the visual representation and internal structure of a GUI. It filters accessibility information for only visible objects (red boxes) and also provides role, content, location, and size of objects detected by pixel-based methods (green boxes). "2011c,A Case for Query by Image and Text Content: Searching Computer Help Using Screenshots and Keywords,Tom Yeh; Brandyn White; Jose San Pedro; Boris Katz; Larry Davis,Conference Paper,WWW,2011,12.5,,http://dl.acm.org/citation.cfm?id=1963405.1963513,"The multimedia information retrieval community has dedicated extensive research effort to the problem of content-based image retrieval (CBIR). However, these systems find their main limitation in the difficulty of creating pictorial queries. As a result, few systems offer the option of querying by visual examples, and rely on automatic concept detection and tagging techniques to provide support for searching visual content using textual queries.<p>This paper proposes and studies a practical multimodal web search scenario, where CBIR fits intuitively to improve the retrieval of rich information queries. Many online articles contain useful know-how knowledge about computer applications. These articles tend to be richly illustrated by screenshots. We present a system to search for such software know-how articles that leverages the visual correspondences between screenshots. Users can naturally create pictorial queries simply by taking a screenshot of the application to retrieve a list of articles containing a matching screenshot.<p>We build a prototype comprising 150k articles that are classified into walkthrough, book, gallery, and general categories, and provide a comprehensive evaluation of this system, focusing on technical (accuracy of CBIR techniques) and usability (perceived system usefulness) aspects. We also consider the study of added value features of such a visual-supported search, including the ability to perform cross-lingual queries. We find that the system is able to retrieve matching screenshots for a wide variety of programs, across language boundaries, and provide subjectively more useful results than keyword-based web and image search engines.",SikuliSearch.png,2010a,VizWiz: Nearly Real-time Answers to Visual Questions,Jeffrey Bigham; Chandrika Jayant; Hanjie Ji; Greg Little; Andrew Miller; Robert Miller; Robin Miller; Aubrey Tatarowicz; Brandyn White; Samuel White; Tom Yeh,Conference Paper,UIST,2010,18,Best Paper Award,http://dl.acm.org/citation.cfm?id=1866029.1866080,"The lack of access to visual information like text labels, icons, and colors can cause frustration and decrease independence for blind people. Current access technology uses automatic approaches to address some problems in this space, but the technology is error-prone, limited in scope, and quite expensive. In this paper, we introduce VizWiz, a talking application for mobile phones that offers a new alternative to answering visual questions in nearly real-time - asking multiple people on the web. To support answering questions quickly, we introduce a general approach for intelligently recruiting human workers in advance called quikTurkit so that workers are available when new questions arrive. A field deployment with 11 blind participants illustrates that blind people can effectively use VizWiz to cheaply answer questions in their everyday lives, highlighting issues that automatic approaches will need to address to be useful. Finally, we illustrate the potential of using VizWiz as part of the participatory design of advanced tools by using it to build and evaluate VizWiz::LocateIt, an interactive mobile tool that helps blind people solve general visual search problems.",vizwiz.png,"The VizWiz client is a talking application for the iPhone 3GS that works with the included VoiceOver screen reader. VizWiz proceeds in three steps: taking a picture, speaking a question, and then waiting for answers. System components include a web server that serves the question to web-based workers, a speech recognition service that converts spoken questions to text, and a database that holds questions and answers. quikTurkit is a separate service that adaptively posts jobs (HITs) to Mechanical Turk in order to maintain specified criteria (for instance, a minimum number of answers per question or a pool of waiting workers of a given size)"2010b,Why Did the Person Cross the Road (There)? Scene Understanding using Probabilistic Logic Models and Common Sense Reasoning,Aniruddha Kembhavi; Tom Yeh; Larry Davis,Conference Paper,ECCV,2010,27.68,,http://dl.acm.org/citation.cfm?id=1888081,"We develop a video understanding system for scene elements, such as bus stops, crosswalks, and intersections, that are characterized more by qualitative activities and geometry than by intrinsic appearance. The domain models for scene elements are not learned from a corpus of video, but instead, naturally elicited by humans, and represented as probabilistic logic rules within a Markov Logic Network framework. Human elicited models, however, represent object interactions as they occur in the 3D world rather than describing their appearance projection in some specific 2D image plane. We bridge this gap by recovering qualitative scene geometry to analyze object interactions in the 3D world and then reasoning about scene geometry, occlusions and common sense domain knowledge using a set of meta-rules. The effectiveness of this approach is demonstrated on a set of videos of public spaces.",ECCV2010.png,"Our scene understanding system consists of an image analysis module (top) that takes an input video and outputs a set of events and zone characteristics as observational evidence, a knowledge base that stores human elicited domain models and general rules about scene geometry and occlusion as a set of Þrst-order logic rules, and an inference engine based on Markov Logic Networks that uses the logic rules and observational evidence to infer the labels of visible scene element."2010c,GUI Testing Using Computer Vision,Tsung-Hsiang Chang; Tom Yeh; Robert Miller,Conference Paper,CHI,2010,22,,http://dl.acm.org/citation.cfm?id=1753326.1753555,"Testing a GUI's visual behavior typically requires human testers to interact with the GUI and to observe whether the expected results of interaction are presented. This paper presents a new approach to GUI testing using computer vision for testers to automate their tasks. Testers can write a visual test script that uses images to specify which GUI components to interact with and what visual feedback to be observed. Testers can also generate visual test scripts by demonstration. By recording both input events and screen images, it is possible to extract the images of components interacted with and the visual feedback seen by the demonstrator, and generate a visual test script automatically. We show that a variety of GUI behavior can be tested using this approach. Also, we show how this approach can facilitate good testing practices such as unit testing, regression testing, and test-driven development.",SikuliTest.png,GUI testing (left) traditionally requires human testers to operate the GUI and verify its behavior visually. Our new testing frame- work allows the testers to write visual scripts (right) to automate this labor-intensive task.2009a,Sikuli: Using GUI Screenshots for Search and Automation,Tom Yeh; Tsung-Hsiang Chang; Robert Miller,Conference Paper,UIST,2009,16.9,Best Paper Award,http://dl.acm.org/citation.cfm?id=1622213,"We present Sikuli, a visual approach to search and automation of graphical user interfaces using screenshots. Sikuli allows users to take a screenshot of a GUI element (such as a toolbar button, icon, or dialog box) and query a help system using the screenshot instead of the element's name. Sikuli also provides a visual scripting API for automating GUI interactions, using screenshot patterns to direct mouse and keyboard events. We report a web-based user study showing that searching by screenshot is easy to learn and faster to specify than keywords. We also demonstrate several automation tasks suitable for visual scripting, such as map navigation and bus tracking, and show how visual scripting can improve interactive help systems previously proposed in the literature.",Sikuli.png,Sikuli Search allows users to search docu- mentation and save custom annotations for a GUI element using its screenshot (captured by stretching a rectangle around it). Sikuli Script allows users to au- tomate GUI interactions also using screenshots.2009b,Fast Concurrent Object Localization and Recognition,Tom Yeh; John Lee; Trevor Darrell,Conference Paper,CVPR,2009,26.16,,http://dx.doi.org/10.1109/CVPR.2009.5206805,"Object localization and recognition are important problems in computer vision. However, in many applications, exhaustive search over all object models and image locations is computationally prohibitive. While several methods have been proposed to make either recognition or localization more efficient, few have dealt with both tasks simultaneously. This paper proposes an efficient method for concurrent object localization and recognition based on a data-dependent multi-class branch-and-bound formalism. Existing bag-of-features recognition techniques which can be expressed as weighted combinations of feature counts can be readily adapted to our method. We present experimental results that demonstrate the merit of our algorithm in terms of recognition accuracy, localization accuracy, and speed, compared to baseline approaches including exhaustive search, implicit-shape model (ISM), and efficient sub-window search (ESS). Moreover, we develop two extensions to consider non-rectangular bounding regions-composite boxes and polygons-and demonstrate their ability to achieve higher recognition scores compared to traditional rectangular bounding boxes.",fastConcurrent.png,Searching for the best composite bounding box (left) and bounding polygon (right).2008a,Photo-based Question Answering ,Tom Yeh; John Lee; Trevor Darrell,Conference Paper,ACM Multimedia,2008,17,,http://dl.acm.org/citation.cfm?id=1459412,"Photo-based question answering is a useful way of finding information about physical objects. Current question answering (QA) systems are text-based and can be difficult to use when a question involves an object with distinct visual features. A photo-based QA system allows direct use of a photo to refer to the object. We develop a three-layer system architecture for photo-based QA that brings together recent technical achievements in question answering and image matching. The first, template-based QA layer matches a query photo to online images and extracts structured data from multimedia databases to answer questions about the photo. To simplify image matching, it exploits the question text to filter images based on categories and keywords. The second, information retrieval QA layer searches an internal repository of resolved photo-based questions to retrieve relevant answers. The third, human-computation QA layer leverages community experts to handle the most difficult cases. A series of experiments performed on a pilot dataset of 30,000 images of books, movie DVD covers, grocery items, and landmarks demonstrate the technical feasibility of this architecture. We present three prototypes to show how photo-based QA can be built into an online album, a text-based QA, and a mobile application.",photoQA.png,"Three-layer system architecture for photo-based QA. For a simple question, the template-based layer identifies its category (e.g., building), finds a matched image within the category, and forms a template to extract an answer from the Web. For a harder question, the IR-based layer searches for relevant photo-based questions already resolved. The human-based layer handles the rest of the questions too difficult for the first two layers."2008b,Dynamic Visual Category Learning ,Tom Yeh; Trevor Darrell,Conference Paper,CVPR,2008,27.9,,http://dx.doi.org/10.1109/CVPR.2008.4587616,"Dynamic visual category learning calls for efficient adaptation as new training images become available or new categories are defined, existing training images or categories become modified or obsolete, or when categories are divided into subcategories or merged together. We develop novel methods for efficient incremental learning of SVM-based visual category classifiers to handle such dynamic tasks. Our method exploits previous classifier estimates to more efficiently learn the optimal parameters for the current set of training images and categories. We show empirically that for dynamic visual category tasks, our incremental learning methods are significantly faster than batch retraining.",,2007,Adaptive Vocabulary Forests for Dynamic Indexing and Category Learning ,Tom Yeh; John Lee; Trevor Darrell,Conference Paper,ICCV,2007,23.4,,http://dx.doi.org/10.1109/ICCV.2007.4409053,"Histogram pyramid representations computed from a vocabulary tree of visual words have proven valuable for a range of image indexing and recognition tasks; however, they have only used a single, fixed partition of feature space. We present a new efficient algorithm to incrementally compute set-of-trees (forest) vocabulary representations, and show that they improve recognition and indexing performance in methods which use histogram pyramids. Our algorithm incrementally adapts a vocabulary forest with an Inverted file system at the leaf nodes and automatically keeps existing histogram pyramid database entries up-to-date in a forward filesystem. It is possible not only to apply vocabulary tree indexing algorithms directly, but also to compute pyramid match kernel values efficiently. On dynamic recognition tasks where categories or objects under consideration may change over time, we show that adaptive vocabularies offer significant performance advantages in comparison to a single, fixed vocabulary.",,2004a,IDeixis: Image-based Deixis for Finding Location-Based Information ,Konrad Tollmar; Tom Yeh; Trevor Darrell ,Conference Paper,MobileHCI,2004,,,http://www.springerlink.com/content/dj2pvjwhgxx1vp1p/,"In this paper we describe an image-based approach to finding location-based information from camera-equipped mobile devices. We introduce a point-by-photograph paradigm, where users can specify a location simply by taking pictures. Our technique uses content-based image retrieval methods to search the web or other databases for matching images and their source pages to find relevant location-based information. In contrast to conventional approaches to location detection, our method can refer to distant locations and does not require any physical infrastructure beyond mobile internet service. We have developed a prototype on a camera phone and conducted user studies to demonstrate the efficacy of our approach.",,2004b,Searching the Web with Mobile Images for Location Recognition ,Tom Yeh; Konrad Tollmar; Trevor Darrell ,Conference Paper,CVPR,2004,23.6,,http://dx.doi.org/10.1109/CVPR.2004.1315147,"We describe an approach to recognizing location from mobile devices using image-based Web search. We demonstrate the usefulness of common image search metrics applied on images captured with a camera-equipped mobile device to find matching images on the World Wide Web or other general-purpose databases. Searching the entire Web can be computationally overwhelming, so we devise a hybrid image-and-keyword searching technique. First, image-search is performed over images and links to their source Web pages in a database that indexes only a small fraction of the Web. Then, relevant keywords on these Web pages are automatically identified and submitted to an existing text-based search engine (e.g. Google) that indexes a much larger portion of the Web. Finally, the resulting image set is filtered to retain images close to the original query. It is thus possible to efficiently search hundreds of millions of images that are not only textually related but also visually relevant. We demonstrate our approach on an application allowing users to browse Web pages matching the image of a nearby location.",,2011d,Co-designing Contextual Tutorials for Older Adults on Searching Health Information on the Internet,Bo Xie; Tom Yeh; Greg Walsh; Ivan Watkins; Man Huang,Short Paper,ASIS&T,2011,,,,"Older adults' e-health literacy, or the ability to access and use reliable health information through electronic sources, is generally low. Improving older adults' e-health literacy requires innovative instructional approaches and tools. An integrated e-tutorial displays instructions as an overlay on the actual Website. Evidence in the literature suggests an integrated e-tutorial is more effective than a paper- or video-based tutorial among younger people. Yet, relatively little is known about the effectiveness of an integrated e-tutorial on the older population. This exploratory study began to explore the applicability of an integrated e-tutorial to an older population, focusing on the content area of e-health literacy. A specific integrated e-tutorial, the Online Tutorial Overlay Presenter (OnTOP), was used to add an overlay to the NIHSeniorHealth.gov Website. Features of the overlay were examined thoroughly in seven 2-hour-long participatory design sessions with ten older adults during November 2010-March 2011. Several participatory design techniques were used to elicit participants' preferences for design features of the OnTOP tutorial. These techniques included drawing on the board, voice recording and integration, and peer instruction. Three major themes were identified, including: 1) using contextual cues, including gestural and auditory cues, to facilitate learning; 2) tailoring to accommodate the learner's literacy level; and 3) enhancing existing interfaces by adding multimedia cues (e.g., using images of everyday objects to replace unfamiliar abstract computer symbols). These findings helped improve the design features of the OnTOP. They also contribute to the multimedia learning literature by generating empirical evidence about the effects of multimedia learning among the previously understudied older adult population, and by raising interesting questions worthy of further examination.",,2008c,Multimodal Question Answering for Mobile Devices ,Tom Yeh; Trevor Darrell,Short Paper,IUI,2008,38,,http://dl.acm.org/citation.cfm?id=1378841,"This paper introduces multimodal question answering, a new interface for community-based question answering services. By offering users an extra modality---photos---in addition to the text modality to formulate queries, multimodal question answering overcomes the limitations of text-only input methods when the users ask questions regarding visually distinctive objects. Such interface is especially useful when users become curious about an interesting object in the environment and want to know about it---simply by taking a photo and asking a question in a situated (from a mobile device) and intuitive (without describing the object in words) manner. We propose a system architecture for multimodal question answering, describe an algorithm for searching the database, and report on the findings of two prototype studies.",,2005a,A Picture is Worth a Thousand Keywords: Image-based Object Search on a Mobile Platform ,Tom Yeh; Kristen Grauman; Konrad Tollmar; Trevor Darrell,Short Paper,CHI,2005,33,,http://dl.acm.org/citation.cfm?id=1057083,"Finding information based on an object's visual appearance is useful when specific keywords for the object are not known. We have developed a mobile image-based search system that takes images of objects as queries and finds relevant web pages by matching them to similar images on the web. Image-based search works well when matching full scenes, such as images of buildings or landmarks, and for matching objects when the boundary of the object in the image is available. We demonstrate the effectiveness of a simple interactive paradigm for obtaining a segmented object boundary, and show how a shape-based image matching algorithm can use the object outline to find similar images on the web.",,2005b,DoubleShot: An Interactive User-Aided Segmentation Tool ,Tom Yeh; Trevor Darrell,Short Paper,IUI,2005,34,,http://dl.acm.org/citation.cfm?id=1040901,"In this paper, we describe an intelligent user interface designed for camera phones to allow mobile users to specify the object of interest in the scene simply by taking two pictures: one with the object and one without the object. By comparing these two images, the system can reliably extract the visual appearance of the object, which can be useful to a wide-range of applications such as content-based image retrieval and object recognition.",,2011e,Active Inference for Retrieval in Camera Networks,DaoZheng Chen; Mustafa Bilgic; Lise Getoor; David Jacobs; Lilly Mihalkova; Tom Yeh,Workshop,IEEE Workshop on Person Oriented Vision,2011,,,http://dx.doi.org/10.1109/POV.2011.5712363,"We address the problem of searching camera network videos to retrieve frames containing specified individuals. We show the benefit of utilizing a learned probabilistic model that captures dependencies among the cameras. In addition, we develop an active inference framework that can request human input at inference time, directing human attention to the portions of the videos whose correct annotation would provide the biggest performance improvements. Our primary contribution is to show that by mapping video frames in a camera network onto a graphical model, we can apply collective classification and active inference algorithms to significantly increase the performance of the retrieval system, while minimizing the number of human annotations required.",,2010d,Web-Scale Computer Vision using MapReduce for Multimedia Data Mining,Brandyn White; Tom Yeh; Jimmy Lin; Larry Davis,Workshop,KDD Multimedia Data Mining Workshop,2010,,,http://dl.acm.org/citation.cfm?id=1814254,"This work explores computer vision applications of the MapReduce framework that are relevant to the data mining community. An overview of MapReduce and common design patterns are provided for those with limited MapReduce background. We discuss both the high level theory and the low level implementation for several computer vision algorithms: classifier training, sliding windows, clustering, bag-of-features, background subtraction, and image registration. Experimental results for the k-means clustering and single Gaussian background subtraction algorithms are performed on a 410 node Hadoop cluster.",,2008d,Scalable Classifiers for Internet Vision Tasks ,Tom Yeh; John Lee; Trevor Darrell,Workshop,CVPR Internet Vision Workshop,2008,,,http://dx.doi.org/10.1109/CVPRW.2008.4562958,"Object recognition systems designed for Internet applications typically need to adapt to userspsila needs in a flexible fashion and scale up to very large data sets. In this paper, we analyze the complexity of several multiclass SVM-based algorithms and highlight the computational bottleneck they suffer at test time: comparing the input image to every training image. We propose an algorithm that overcomes this bottleneck; it offers not only the efficiency of a simple nearest-neighbor classifier, by voting on class labels based on the k nearest neighbors quickly determined by a vocabulary tree, but also the recognition accuracy comparable to that of a complex SVM classifier, by incorporating SVM parameters into the voting scores incrementally accumulated from individual image features. Empirical results demonstrate that adjusting votes by relevant support vector weights can improve the recognition accuracy of a nearest-neighbor classifier without sacrificing speed. Compared to existing methods, our algorithm achieves a ten-fold speed increase while incurring an acceptable accuracy loss that can be easily offset by showing about two more labels in the result. The speed, scalability, and adaptability of our algorithm makes it suitable for Internet vision applications.",,2009c,"Searching Documentation Using Text, OCR, and Image",Tom Yeh; Boris Katz,Conference Poster,SIGIR,2009,34,,http://dl.acm.org/citation.cfm?id=1572123,"We describe a mixed-modality method to index and search software documentation in three ways: plain text, OCR text of embedded figures, and visual features of these figures. Using a corpus of 102 computer books with a total of 62,943 pages and 75,800 figures, we empirically demonstrate that our method achieves better precision/recall than do alternatives based on single modalities.",,2008e,Obtaining Help on GUI Elements using Screenshots ,Tom Yeh; Tsung-Hsiang Chang; Robert Miller,Conference Poster,UIST,2008,,,,"We present an image-based help mechanism for users to obtain information about novel and unfamiliar GUI elements. Instead of reading tooltips and/or using keywords to search, users can use the screenshots of the GUI elements directly to retrieve relevant information. Many offline and online resources contain screenshots and text about software applications that can be indexed and searched based on visual similarity. A preliminary experiment on a small pilot dataset has demonstrated the technical feasibility of a state-of-the-art image matching algorithm to handle screenshots",,